{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dede6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fdad1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../graph_data/\"\n",
    "# 1. Load files\n",
    "with open(datapath+'entities.json', 'r') as f:\n",
    "    entities = json.load(f)             # { entity_str: { 'canonical': text, ... }, ... }\n",
    "with open(datapath+'relation2id.json', 'r') as f:\n",
    "    rel2id = json.load(f)               # { relation_str: relation_id, ... }\n",
    "\n",
    "# invert mappings\n",
    "entity2id = {ent: i for i, ent in enumerate(entities.keys())}\n",
    "id2entity = {i: ent for ent, i in entity2id.items()}\n",
    "id2rel    = {rid: rel for rel, rid in rel2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bf908ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_triples(path):\n",
    "    triples = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            h_str, r_id_str, t_str = line.strip().split('\\t')\n",
    "            triples.append((h_str, int(r_id_str), t_str))\n",
    "    return triples\n",
    "\n",
    "\n",
    "\n",
    "train_triples = load_triples(datapath+\"triples_train.tsv\")\n",
    "dev_triples   = load_triples(datapath+\"triples_dev.tsv\")\n",
    "test_triples  = load_triples(datapath+\"triples_test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bbc70ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# def negative_sample_tails(triples, num_entities, neg_rate=1):\n",
    "#     augmented = []\n",
    "#     for h, r, t in triples:\n",
    "#         # positive\n",
    "#         augmented.append((h, r, t, 1))\n",
    "#         # k negatives by replacing t\n",
    "#         for _ in range(neg_rate):\n",
    "#             t_neg = random.randrange(num_entities)\n",
    "#             while t_neg == t:\n",
    "#                 t_neg = random.randrange(num_entities)\n",
    "#             augmented.append((h, r, t_neg, 0))\n",
    "#     return augmented\n",
    "\n",
    "# train_examples = negative_sample_tails(train_triples, len(entity2id), neg_rate=1)\n",
    "# # dev/test: no negatives, so label=1 for all\n",
    "# dev_examples  = [(h, r, t, 1) for (h, r, t) in dev_triples]\n",
    "# test_examples = [(h, r, t, 1) for (h, r, t) in test_triples]\n",
    "\n",
    "\n",
    "def negative_sample_tails(triples, neg_rate=1):\n",
    "    out = []\n",
    "    num_entities = len(entity2id)\n",
    "    for h, r, t in triples:\n",
    "        out.append((h, r, t, 1))\n",
    "        for _ in range(neg_rate):\n",
    "            t_neg = random.randrange(num_entities)\n",
    "            # map back to name\n",
    "            t_neg_str = list(entity2id.keys())[t_neg]\n",
    "            while t_neg_str == t:\n",
    "                t_neg = random.randrange(num_entities)\n",
    "                t_neg_str = list(entity2id.keys())[t_neg]\n",
    "            out.append((h, r, t_neg_str, 0))\n",
    "    return out\n",
    "\n",
    "train_examples = negative_sample_tails(train_triples, neg_rate=1)\n",
    "dev_examples = negative_sample_tails(dev_triples, neg_rate=1)\n",
    "test_examples = negative_sample_tails(test_triples, neg_rate=1)\n",
    "# dev/test: no negatives, so label=1 for all\n",
    "# dev_examples  = [(h, r, t, 1) for (h, r, t) in dev_triples]\n",
    "# test_examples = [(h, r, t, 1) for (h, r, t) in test_triples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e0acc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class KGTailPredDataset(Dataset):\n",
    "    def __init__(self, examples, entities, entity2id, id2rel, tokenizer, max_len=128):\n",
    "        self.examples   = examples\n",
    "        self.entities   = entities\n",
    "        self.entity2id  = entity2id\n",
    "        self.id2rel     = id2rel\n",
    "        self.tokenizer  = tokenizer\n",
    "        self.max_len    = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        h_str, r_id, t_str, label = self.examples[idx]\n",
    "        h_text = self.entities[h_str]['canonical']\n",
    "        r_text = self.id2rel[r_id]\n",
    "        t_text = self.entities[t_str]['canonical']\n",
    "\n",
    "        seq = f\"{h_text} [SEP] {r_text} [SEP] {t_text}\"\n",
    "        enc = self.tokenizer(\n",
    "            seq,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item.update({\n",
    "            'head_id':     torch.tensor(self.entity2id[h_str], dtype=torch.long),\n",
    "            'relation_id': torch.tensor(r_id,                  dtype=torch.long),\n",
    "            'tail_id':     torch.tensor(self.entity2id[t_str], dtype=torch.long),\n",
    "            'label':       torch.tensor(label,                 dtype=torch.float),\n",
    "        })\n",
    "        return item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5611a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Instantiate\n",
    "tokenizer   = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_ds    = KGTailPredDataset(train_examples, entities, entity2id, id2rel, tokenizer)\n",
    "dev_ds      = KGTailPredDataset(dev_examples,   entities, entity2id, id2rel, tokenizer)\n",
    "test_ds     = KGTailPredDataset(test_examples,  entities, entity2id, id2rel, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "dev_loader   = DataLoader(dev_ds,   batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71262ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGBertTailPredictor(nn.Module):\n",
    "    def __init__(self, pretrained_model=\"bert-base-uncased\", dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 1) Backbone\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        # 2) A small head on [CLS]\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Single logit → score “true” vs “false”\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
    "        \"\"\"\n",
    "        - input_ids etc. come from your TailPredDataset\n",
    "        - labels: a float tensor of 0/1 (only in training)\n",
    "        Returns dict with:\n",
    "         • logits: shape (batch,)\n",
    "         • loss: BCEWithLogitsLoss if labels provided\n",
    "        \"\"\"\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        # pooled_output is the [CLS] representation\n",
    "        pooled = outputs.pooler_output            # (batch, hidden_size)\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled).squeeze(-1)  # (batch,)\n",
    "\n",
    "        out = {\"logits\": logits}\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            out[\"loss\"] = loss_fn(logits, labels.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab18e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch in tqdm(train_loader, desc=\"Train\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        out = model(\n",
    "            input_ids      = batch[\"input_ids\"],\n",
    "            attention_mask = batch[\"attention_mask\"],\n",
    "            token_type_ids = batch[\"token_type_ids\"],\n",
    "            labels         = batch[\"label\"],\n",
    "        )\n",
    "        loss = out[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_binary(loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs  = []\n",
    "    for batch in tqdm(loader, desc=\"Eval (binary)\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        out = model(\n",
    "            input_ids      = batch[\"input_ids\"],\n",
    "            attention_mask = batch[\"attention_mask\"],\n",
    "            token_type_ids = batch[\"token_type_ids\"],\n",
    "        )\n",
    "        logits = out[\"logits\"]                 # (batch_size,)\n",
    "        probs  = torch.sigmoid(logits).cpu().numpy()\n",
    "        labels = batch[\"label\"].cpu().numpy()\n",
    "\n",
    "        all_probs.extend(probs)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    # threshold at 0.5\n",
    "    preds = [1 if p > 0.5 else 0 for p in all_probs]\n",
    "\n",
    "    acc = accuracy_score(all_labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, preds, average=\"binary\"\n",
    "    )\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    return acc, prec, rec, f1, auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43c9696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = KGBertTailPredictor().to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "279aac54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 202/202 [00:32<00:00,  6.30it/s]\n",
      "Eval (binary): 100%|██████████| 15/15 [00:01<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 ▶ train_loss=0.6942  dev_acc=0.5736  precision=0.5827  recall=0.5187  f1=0.5488  auc=0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 202/202 [00:32<00:00,  6.24it/s]\n",
      "Eval (binary): 100%|██████████| 15/15 [00:01<00:00,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 ▶ train_loss=0.6730  dev_acc=0.5758  precision=0.5739  recall=0.5890  f1=0.5813  auc=0.6038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 202/202 [00:32<00:00,  6.29it/s]\n",
      "Eval (binary): 100%|██████████| 15/15 [00:01<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 ▶ train_loss=0.6169  dev_acc=0.6077  precision=0.6109  recall=0.5934  f1=0.6020  auc=0.6441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 202/202 [00:32<00:00,  6.28it/s]\n",
      "Eval (binary): 100%|██████████| 15/15 [00:01<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 ▶ train_loss=0.4894  dev_acc=0.5681  precision=0.5714  recall=0.5451  f1=0.5579  auc=0.6107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 202/202 [00:32<00:00,  6.28it/s]\n",
      "Eval (binary): 100%|██████████| 15/15 [00:01<00:00,  9.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 ▶ train_loss=0.3399  dev_acc=0.5791  precision=0.5763  recall=0.5978  f1=0.5868  auc=0.6214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 202/202 [00:32<00:00,  6.25it/s]\n",
      "Eval (binary): 100%|██████████| 15/15 [00:01<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 ▶ train_loss=0.2179  dev_acc=0.5879  precision=0.5985  recall=0.5341  f1=0.5645  auc=0.6276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 202/202 [00:32<00:00,  6.17it/s]\n",
      "Eval (binary): 100%|██████████| 15/15 [00:01<00:00,  9.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 ▶ train_loss=0.1259  dev_acc=0.5659  precision=0.5714  recall=0.5275  f1=0.5486  auc=0.6099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 202/202 [00:32<00:00,  6.20it/s]\n",
      "Eval (binary): 100%|██████████| 15/15 [00:01<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 ▶ train_loss=0.0763  dev_acc=0.5758  precision=0.5860  recall=0.5165  f1=0.5491  auc=0.6165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 202/202 [00:31<00:00,  6.43it/s]\n",
      "Eval (binary): 100%|██████████| 15/15 [00:01<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 ▶ train_loss=0.0527  dev_acc=0.5692  precision=0.5734  recall=0.5407  f1=0.5566  auc=0.6127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 202/202 [00:31<00:00,  6.45it/s]\n",
      "Eval (binary): 100%|██████████| 15/15 [00:01<00:00, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 ▶ train_loss=0.0347  dev_acc=0.5725  precision=0.5778  recall=0.5385  f1=0.5575  auc=0.6099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Main loop ---\n",
    "best_dev_auc = 0.0\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    avg_loss = train_epoch()\n",
    "    acc, prec, rec, f1, auc = eval_binary(dev_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} ▶ train_loss={avg_loss:.4f}  \"\n",
    "          f\"dev_acc={acc:.4f}  precision={prec:.4f}  recall={rec:.4f}  f1={f1:.4f}  auc={auc:.4f}\")\n",
    "\n",
    "    # save best by AUC (or any metric you choose)\n",
    "    if auc > best_dev_auc:\n",
    "        best_dev_auc = auc\n",
    "        torch.save(model.state_dict(), \"best_kgbt_tailpred.pt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b18207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jerry\\AppData\\Local\\Temp\\ipykernel_30464\\2734437120.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_kgbt_tailpred.pt\"))\n",
      "Eval (binary): 100%|██████████| 31/31 [00:03<00:00,  9.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ▶ acc=0.5637  precision=0.5628  recall=0.5708  f1=0.5668  auc=0.6064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Final test evaluation ---\n",
    "model.load_state_dict(torch.load(\"best_kgbt_tailpred.pt\"))\n",
    "acc, prec, rec, f1, auc = eval_binary(test_loader)\n",
    "print(f\"Test ▶ acc={acc:.4f}  precision={prec:.4f}  recall={rec:.4f}  f1={f1:.4f}  auc={auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb9a46",
   "metadata": {},
   "source": [
    "Not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b456275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_epoch():\n",
    "#     model.train()\n",
    "#     losses = []\n",
    "#     for batch in tqdm(train_loader, desc=\"Train\"):\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#         out = model(\n",
    "#             input_ids      = batch[\"input_ids\"],\n",
    "#             attention_mask = batch[\"attention_mask\"],\n",
    "#             token_type_ids = batch[\"token_type_ids\"],\n",
    "#             labels         = batch[\"label\"],\n",
    "#         )\n",
    "#         loss = out[\"loss\"]\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         losses.append(loss.item())\n",
    "#     return np.mean(losses)\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def evaluate(triples):\n",
    "#     model.eval()\n",
    "#     ranks = []\n",
    "#     # for each true (h,r,t), score all candidate tails\n",
    "#     for h_id, r_id, t_id in tqdm(triples, desc=\"Eval\"):\n",
    "#         h_text = entities[h_id][\"canonical\"]\n",
    "#         r_text = id2rel[r_id]\n",
    "\n",
    "#         # build all candidate sequences in one batch\n",
    "#         seqs = [\n",
    "#             f\"{h_text} [SEP] {r_text} [SEP] {entities[id2entity[t2]]['canonical']}\"\n",
    "#             for t2 in range(len(entity2id))\n",
    "#         ]\n",
    "#         enc = tokenizer(\n",
    "#             seqs,\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=128,\n",
    "#             return_tensors=\"pt\",\n",
    "#         ).to(device)\n",
    "\n",
    "#         logits = model(\n",
    "#             input_ids      = enc.input_ids,\n",
    "#             attention_mask = enc.attention_mask,\n",
    "#             token_type_ids = enc.token_type_ids,\n",
    "#         )[\"logits\"]              # (num_entities,)\n",
    "\n",
    "#         scores = torch.sigmoid(logits).cpu()\n",
    "#         # get descending ranking\n",
    "#         sorted_idx = torch.argsort(scores, descending=True)\n",
    "#         rank = (sorted_idx == entity2id[t_id]).nonzero(as_tuple=False).item() + 1\n",
    "#         ranks.append(rank)\n",
    "\n",
    "#     ranks = np.array(ranks)\n",
    "#     mrr    = np.mean(1.0 / ranks)\n",
    "#     hits1  = np.mean(ranks <= 1)\n",
    "#     hits3  = np.mean(ranks <= 3)\n",
    "#     hits10 = np.mean(ranks <= 10)\n",
    "#     return mrr, {\"hits@1\": hits1, \"hits@3\": hits3, \"hits@10\": hits10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbdf63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\jerry\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = KGBertTailPredictor(\"distilbert-base-uncased\").to(device)\n",
    "\n",
    "# num_epochs = 1\n",
    "# # optimizer + optional scheduler\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "# total_steps = len(train_loader) * num_epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(\n",
    "#     optimizer,\n",
    "#     num_warmup_steps=int(0.1 * total_steps),\n",
    "#     num_training_steps=total_steps,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f118a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:   0%|          | 0/455 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:   0%|          | 1/455 [01:06<8:23:10, 66.50s/it]"
     ]
    }
   ],
   "source": [
    "# # --- Main training loop ---\n",
    "# best_dev_mrr = 0.0\n",
    "# for epoch in range(1, num_epochs+1):\n",
    "#     # avg_train_loss = train_epoch()\n",
    "#     dev_mrr, dev_hits = evaluate(dev_triples)\n",
    "#     print(f\"Epoch {epoch:02d}  train_loss={avg_train_loss:.4f}  dev_mrr={dev_mrr:.4f}  hits@10={dev_hits['hits@10']:.4f}\")\n",
    "\n",
    "#     # save best\n",
    "#     if dev_mrr > best_dev_mrr:\n",
    "#         best_dev_mrr = dev_mrr\n",
    "#         torch.save(model.state_dict(), \"best_kgbt_tailpred.pt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81110d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Final test evaluation ---\n",
    "# model.load_state_dict(torch.load(\"best_kgbt_tailpred.pt\"))\n",
    "# test_mrr, test_hits = evaluate(test_triples)\n",
    "# print(f\"Test MRR: {test_mrr:.4f}  Hits@1: {test_hits['hits@1']:.4f}  Hits@3: {test_hits['hits@3']:.4f}  Hits@10: {test_hits['hits@10']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
